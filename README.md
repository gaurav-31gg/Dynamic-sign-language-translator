# Dynamic-sign-language-translator
# **Sign Language Recognition System** ‚úãüî§  

## **Overview**  
This project is a **real-time sign language recognition system** that enables seamless communication for individuals using sign language. The system utilizes a **CNN-based model** to recognize hand gestures via a webcam and translates them into meaningful outputs.  

Built using **TensorFlow.js, MobileNet, and KNN**, this browser-based application processes live video feed, extracts hand gesture features, and classifies them accurately. The project aims to bridge the communication gap between the hearing-impaired and others through innovative machine learning techniques.  

---

## **‚ú® Features**  
‚úÖ **Real-time hand gesture recognition** using a webcam  
‚úÖ **Fully browser-based** (no additional software installation required)  
‚úÖ **Uses MobileNet for efficient feature extraction**  
‚úÖ **K-Nearest Neighbors (KNN) for gesture classification**  
‚úÖ **Lightweight and optimized for smooth performance**  
‚úÖ **User-friendly UI with seamless interaction**  

---

## **üõ†Ô∏è Technologies Used**  
### **Frontend:**  
- **HTML, CSS, JavaScript** for building the UI  
- **OpenCV.js** for image processing and hand tracking  

### **Machine Learning & Gesture Recognition:**  
- **TensorFlow.js** for deep learning in the browser  
- **MobileNet** for extracting hand gesture features  
- **K-Nearest Neighbors (KNN)** for gesture classification  

---

## **üìñ How It Works**  
The system follows a structured pipeline for gesture recognition:  

### **1Ô∏è‚É£ Webcam Activation & Hand Tracking** üé•  
- The application **captures a live video feed** using the webcam.  
- OpenCV.js processes the frames and **removes the background** using the **HSV color space** for better gesture isolation.  

### **2Ô∏è‚É£ Feature Extraction Using MobileNet** üß†  
- MobileNet, a **lightweight deep learning model**, extracts key features from the captured hand image.  
- It converts the image into a compact representation that highlights important characteristics while reducing unnecessary details.  

### **3Ô∏è‚É£ Gesture Classification with KNN** üîç  
- The **K-Nearest Neighbors (KNN) algorithm** classifies the gesture by comparing it with previously labeled samples.  
- It finds the closest match and assigns the recognized gesture.  

### **4Ô∏è‚É£ Displaying Recognized Gesture** ‚úÖ  
- The recognized sign is **displayed as text** in the frontend UI.  
- The system provides a real-time, smooth recognition experience.  

---

## **üöÄ How to Run the Project**  
Follow these steps to set up and run the project on your local machine:  

### **1Ô∏è‚É£ Clone the Repository**  
Open a terminal and run:  
```bash
git clone https://github.com/your-username/sign-language-recognition.git
cd sign-language-recognition
